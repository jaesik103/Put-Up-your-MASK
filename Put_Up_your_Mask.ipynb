{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 마스크 착용 판별기 (얼굴인식)\n",
    "\n",
    "### 1. webcam을 통해 10프레임마다 이미지 저장하여 모델의 데이터셋 생성\n",
    "\n",
    "### 2. InceptionResNetV2 통해 마스크 착용 판별기 훈련\n",
    "1. InceptionResNetV2 Weight 그대로사용, 마지막 레이어만 제거\n",
    "2. fine tuning\n",
    "    - layer 추가\n",
    "    - lr 10~0.00001\n",
    "    - epochs, batch_size\n",
    "    - val_loss 값의 변화로 overfitting 방지\n",
    "\n",
    "### 3. 영상으로 결과값 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. 사용된 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvlib as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvlib as cv\n",
    " \n",
    "# open webcam (웹캠 열기)\n",
    "webcam = cv2.VideoCapture(0)\n",
    " \n",
    "if not webcam.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "    \n",
    " \n",
    "sample_num = 0 # 데이터 수집 시작\n",
    "captured_num = 0 # 데이터 수집 시작\n",
    "    \n",
    "# loop through frames\n",
    "while webcam.isOpened():\n",
    "    \n",
    "    # read frame from webcam \n",
    "    status, frame = webcam.read()\n",
    "    sample_num = sample_num + 1\n",
    "    \n",
    "    if not status:\n",
    "        break\n",
    " \n",
    "    # 이미지 내 얼굴 검출\n",
    "    face, confidence = cv.detect_face(frame)\n",
    "    \n",
    "    #print(face)\n",
    "    #print(confidence)\n",
    " \n",
    "    # loop through detected faces\n",
    "    for idx, f in enumerate(face):\n",
    "        \n",
    "        (startX, startY) = f[0], f[1]\n",
    "        (endX, endY) = f[2], f[3]\n",
    " \n",
    " \n",
    "        if sample_num % 10  == 0:\n",
    "            captured_num = captured_num + 1\n",
    "            face_in_img = frame[startY:endY, startX:endX, :]\n",
    "            cv2.imwrite('./mask/face'+str(captured_num)+'.jpg', face_in_img) # 마스크 착용 데이터 수집시 주석해제\n",
    "            #cv2.imwrite('./nomask/face'+str(captured_num)+'.jpg', face_in_img) # 마스크 미착용 데이터 수집시 주석해제\n",
    " \n",
    " \n",
    "    # display output\n",
    "    cv2.imshow(\"captured frames\", frame)        \n",
    "    \n",
    "    # q 누르면 종료 or 데이터 캡처 완료하면 종료\n",
    "    if cv2.waitKey(1) == ord('q') or captured_num == 300 : # 원하는 샘플 수 \n",
    "        break\n",
    "    \n",
    "# release resources\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    " \n",
    " \n",
    "path_dir1 = './nomask/'\n",
    "path_dir2 = './mask/'\n",
    " \n",
    "file_list1 = os.listdir(path_dir1) # path에 존재하는 파일 목록 가져오기\n",
    "file_list2 = os.listdir(path_dir2)\n",
    " \n",
    "file_list1_num = len(file_list1)\n",
    "file_list2_num = len(file_list2)\n",
    " \n",
    "file_num = file_list1_num + file_list2_num\n",
    " \n",
    " \n",
    "# 이미지 전처리\n",
    "num = 0\n",
    "all_img = np.float32(np.zeros((file_num, 224, 224, 3))) \n",
    "all_label = np.float32(np.zeros((file_num, 1)))\n",
    " \n",
    "for img_name in file_list1:\n",
    "    img_path = path_dir1+img_name\n",
    "    img = load_img(img_path, target_size=(224, 224))\n",
    "    \n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    all_img[num, :, :, :] = x\n",
    "    \n",
    "    all_label[num] = 0 # nomask\n",
    "    num = num + 1\n",
    " \n",
    "for img_name in file_list2:\n",
    "    img_path = path_dir2+img_name\n",
    "    img = load_img(img_path, target_size=(224, 224))\n",
    "    \n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    all_img[num, :, :, :] = x\n",
    "    \n",
    "    all_label[num] = 1 # mask\n",
    "    num = num + 1\n",
    " \n",
    " \n",
    "# 데이터셋 섞기(적절하게 훈련되게 하기 위함) \n",
    "np.random.seed(1004)\n",
    "n_elem = all_label.shape[0]\n",
    "indices = np.random.choice(n_elem, size=n_elem, replace=False)\n",
    " \n",
    "all_label = all_label[indices]\n",
    "all_img = all_img[indices]\n",
    " \n",
    " \n",
    "# 훈련셋 테스트셋 분할\n",
    "num_train = int(np.round(all_label.shape[0]*0.8))\n",
    "num_test = int(np.round(all_label.shape[0]*0.2))\n",
    " \n",
    "train_img = all_img[0:num_train, :, :, :]\n",
    "test_img = all_img[num_train:, :, :, :] \n",
    " \n",
    "train_label = all_label[0:num_train]\n",
    "test_label = all_label[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 학습모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create the base pre-trained model\n",
    "IMG_SHAPE = (224, 224, 3)\n",
    " \n",
    "base_model = InceptionResNetV2(input_shape=IMG_SHAPE, weights='imagenet', include_top=False)\n",
    "base_model.trainable = False\n",
    "#base_model.summary()\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    " \n",
    "flatten_layer = Flatten()\n",
    "dense_layer1 = Dense(128, activation='relu')\n",
    "bn_layer1 = BatchNormalization()\n",
    "dense_layer2 = Dense(1, activation=tf.nn.sigmoid)\n",
    " \n",
    "model = Sequential([\n",
    "        base_model,\n",
    "        flatten_layer,\n",
    "        dense_layer1,\n",
    "        bn_layer1,\n",
    "        dense_layer2,\n",
    "        ])\n",
    " \n",
    "#early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1)\n",
    "\n",
    "base_learning_rate = [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for l in base_learning_rate:\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=l), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_img, train_label, epochs=20, batch_size=16, validation_data = (test_img, test_label))\n",
    "    model.save(\"model_pract{}.h5\".format(l))\n",
    "\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    #결과를 그래프로 표현하는 부분입니다.\n",
    "    \n",
    "    acc= history.history['accuracy']\n",
    "    val_acc= history.history['val_accuracy']\n",
    "    y_vloss = history.history['val_loss']\n",
    "    y_loss = history.history['loss']\n",
    "\n",
    "    x_len = np.arange(len(y_loss)) \n",
    "    plt.suptitle(l)\n",
    "    plt.subplot(1, 2, 1)                # nrows=1, ncols=1, index=1\n",
    "    plt.plot(x_len, y_vloss, marker='.', c=\"cornflowerblue\", label='Testset_loss')\n",
    "    plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
    "    plt.title('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.subplot(1, 2, 2)                # nrows=1, ncols=2, index=2\n",
    "    plt.plot(x_len, val_acc, marker='.', c=\"lightcoral\", label='Testset_acc')\n",
    "    plt.plot(x_len, acc, marker='.', c=\"red\", label='Trainset_acc')\n",
    "    plt.title('acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('acc')\n",
    "    plt.legend(loc='lower right') \n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 마스크 착용 판별기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvlib as cv\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    " \n",
    "model = load_model('model_colab_lr_0.001.h5') # colab 딥러닝 모델 불러오기\n",
    "# model.summary()\n",
    " \n",
    "# open webcam\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "# 동영상 저장용 코드\n",
    "w = round(webcam.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = round(webcam.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = webcam.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, fps*0.4, (w, h))\n",
    " \n",
    "if not webcam.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "    \n",
    " \n",
    "# loop through frames\n",
    "while webcam.isOpened():\n",
    " \n",
    "    # read frame from webcam \n",
    "    status, frame = webcam.read()\n",
    "    \n",
    "    if not status:\n",
    "        print(\"Could not read frame\")\n",
    "        exit()\n",
    " \n",
    "    # apply face detection\n",
    "    face, confidence = cv.detect_face(frame)\n",
    " \n",
    "    # loop through detected faces\n",
    "    for idx, f in enumerate(face):\n",
    "        \n",
    "        (startX, startY) = f[0], f[1]\n",
    "        (endX, endY) = f[2], f[3]\n",
    "        \n",
    "        if 0 <= startX <= frame.shape[1] and 0 <= endX <= frame.shape[1] and 0 <= startY <= frame.shape[0] and 0 <= endY <= frame.shape[0]:\n",
    "            \n",
    "            face_region = frame[startY:endY, startX:endX]\n",
    "            \n",
    "            face_region1 = cv2.resize(face_region, (224, 224), interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "            x = img_to_array(face_region1)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = preprocess_input(x)\n",
    "            \n",
    "            prediction = model.predict(x)\n",
    " \n",
    "            if prediction < 0.5: # 마스크 미착용으로 판별되면\n",
    "                cv2.rectangle(frame, (startX,startY), (endX,endY), (0,0,255), 2)\n",
    "                Y = startY - 10 if startY - 10 > 10 else startY + 20\n",
    "                text = \"No Mask ({:.2f}%)\".format((1 - prediction[0][0])*100)\n",
    "                cv2.putText(frame, text, (startX,Y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "                \n",
    "            else: # 마스크 착용으로 판별되면\n",
    "                cv2.rectangle(frame, (startX,startY), (endX,endY), (0,255,0), 2)\n",
    "                Y = startY - 10 if startY - 10 > 10 else startY + 20\n",
    "                text = \"Mask ({:.2f}%)\".format(prediction[0][0]*100)\n",
    "                cv2.putText(frame, text, (startX,Y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "                \n",
    "    # display output\n",
    "    cv2.imshow(\"mask nomask classify\", frame)\n",
    "    out.write(frame)\n",
    "    # press \"Q\" to stop\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "    \n",
    "# release resources\n",
    "webcam.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d0d1946ff7ae491cc79c40f02d4f4d9f725ccb94ab0ae545a78f1bbd098a817"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('project_01_20220307')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
